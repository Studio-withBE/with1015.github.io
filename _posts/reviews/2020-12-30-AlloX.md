---
title: "AlloX: Compute Allocation in Hybrid Clusters"
author: Hyunjoon Jeong
layout: post
category: review
---
본 리뷰는 EuroSys'20에 게재된 논문 "AlloX: Compute Allocation in Hybrid Clusters"를 읽고 내용을 간단히 정리하고자 쓰였습니다.  
(틀린 내용이나 잘못 이해하고 있는 부분이 있다면 지적 부탁 드리겠습니다.)

AlloX의 motivation은 딥러닝 프레임워크에 대한 "interchangeable resource"로부터 시작합니다.  
보통 기존의 딥러닝은 GPU에서 주로 처리가 되었으나 구글의 Tensor Processing Unit (이하 TPU) 같은 장치들이 Tensorflow의 지원을 받기 시작하면서 딥러닝 클러스터가 heterogeneous computation device로 구성되게 되었습니다. 이러한 클러스터에서 장치들은 interchangeability를 가지게 되지만 이 때문에 선택되는 configuration으로 인해 딥러닝 작업별로 processing time이나 throughput에 큰 차이를 보이게 됩니다. 이에따라 AlloX의 저자들은 다음과 같은 motivation을 추가로 제시했습니다.  

1. Application 단위에서의 interchangeable resource별 성능
2. CPU와 같은 일부 resource의 under-utilization 문제
3. 기존에 존재하는 scheduling policy의 비효율성

<center><img src="/assets/images/allox/allox_01.jpg" width="80%" height="80%"></center>

위 그림과 같이 Bi-LSTM이나 video analytics 같은 작업은 GPU와 CPU를 사용하였을 때 성능이 거의 차이가 없거나 오히려 CPU에서의 작업이 더 나은 성능을 보입니다.  

<center><img src="/assets/images/allox/allox_02.jpg" width="40%" height="40%"> <img src="/assets/images/allox/allox_03.jpg" width="50%" height="50%"></center>

또한 위 그래프는 두번째 motivation에서 언급한 바와 같이 Azure 사용자들의 CPU utilization이 대부분 20%에 못미치는 것으로 일부 resource의 under-utilization 문제를 뒷받침 해주고 있습니다. 이러한 under-utilization 문제를 해결하기 위한 방법으로 CPU와 그 외 성능에 더 유리한 resource들을 두고 적절히 스케줄링하는 방법이 있을 것입니다. 하지만 AlloX의 저자들은 기존의 스케줄링 정책은 multi-resource에 대해서는 효율적이지 않다고 언급합니다. 모든 작업들의 processing time을 알고 있다는 전제 하에 저자들은 First-Come-First-Serve (FCFS), Equal Share (ES), Best Fit (BF), Shortest Job First (SJF), Join the Shortest Queue (JSQ)등의 스케줄링 정책을 시도해보았고, 모든 작업이 끝나는 시간을 기준으로 했을 때, optimal solution과 60%까지 차이가 나타났습니다. 특히 위 SJF 그래프의 경우, 이는 작업에 걸리는 시간을 안다는 전제로 single resource에서 가장 이상적인 스케줄링 정책이지만, multi-resource 환경에서는 오히려 optimal 하지 않다는 것을 보여줍니다.  
결론적으로 AlloX의 저자들이 목표로 하는 것은 다음과 같았습니다.

1. 각각의 딥러닝 작업에 대해 어떤 configuration을 갖고 어떤 순서로 작업을 해야 하는가?
2. fairness나 작업의 길이에 구애받지 않고 어떻게 전체 작업 완료 시간을 최소화 할 수 있는가?

위 2가지 목표를 달성하기 위해서, 저자들은 AlloX의 시스템 구조를 아래 그림과 같이 "Estimator", "Scheduler", "Placer"로 나누어 설계하게 됩니다.  

<center><img src="/assets/images/allox/allox_04.jpg" width="40%" height="40%"></center>  

AlloX 스케줄러의 경우, 각 작업들에 대해 processing time을 Estimator에서 각각의 configuration에 대해 추정하게 됩니다. 이 때 사용하는 방법은 실제 작업에 3% 부분만 사전에 미리 실행시키고 이를 통해 실제 작업에 시간이 얼마나 걸리는지 추정을 하게 됩니다. 물론 이와 같은 방법은 어느 정도 오차를 동반하게 되지만, AlloX의 저자들이 40개의 작업에 대해 추정 오류를 측정한 결과, 평균적으로 8% 정도의 오차가 발생했고, 약 50% 정도의 작업들이 오차가 0에 수렴했다고 주장합니다.  
AlloX 스케줄러가 각 작업에 대해 configuration과 순서를 부여하는 방식은 Min-Cost Bipartite matching 문제를 푸는 것으로 해결합니다. 또한 Estimator에서 얻은 각 configuration에 대한 추정 processing time은 scheduler 단계에서 processing matrix 형태로 표현이 됩니다.  

<center><img src="/assets/images/allox/allox_05.jpg" width="40%" height="40%"></center>  

위 그림은 processing matrix에 대한 예시를 나타냅니다. matrix P의 행은 Job을, 열은 configuration을 의미합니다. 따라서 위 그림에 matrix P는 Job 1이 GPU에서 실행했을 때 3초, CPU에서는 4초가 걸리게 된다는 것을 의미합니다. AlloX는 그 다음으로 total job completion time을 계산하기 위해서 k-th last job이라는 개념을 사용합니다.  
오른쪽 그림에 의하면 각 작업들을 GPU에서 순서대로 실행을 하는 경우, 3초, 7초, 12초가 걸리게 되면서 total completion time은 이들의 합인 22초가 됩니다. k-th last job은 total completion time을 오로지 waiting time의 관점을 통해 계산하는 방식을 의미합니다. 각 작업은 k-th last job의 index로 실행 순서의 역순으로 계산이 됩니다. 따라서 3번째로 실행된 Job 3는 k-th last job의 관점에서는 k=1로 바뀌게 됩니다. 첫번째로 실행되었던 Job 1의 경우는 k-th last job에서는 k=3이 될 것입니다. 또한 첫번째 작업인 Job 1의 경우, Job 2와 Job 3의 waiting time에 영향을 주게 됩니다. Job 1, 2, 3의 total completion time은 3 + (3 + 4) + (3 + 4 + 5) = 22이고, 이를 k-th last job 관점으로 계산을 하게 되면, (k = 1) * 5 + (k = 2) * 4 + (k = 3) * 3 = 22로 기존의 방법과 같은 결과를 나타내게 됩니다.  
결론적으로 total completion time은 k-th last job의 k 값과 해당 k 값을 가지는 processing time의 곱의 합으로 나타낼 수 있게 됩니다.  
자, 그러면 다시 AlloX 스케줄러로 돌아와서, AlloX의 목표는 결국 total completion time을 최소화 시키는 것이 목표 입니다. (물론 interchangeable resource가 있는 경우)  
AlloX의 스케줄러는 processing matrix를 작업의 개수만큼 확장시켜서 k-th last job이 포함된 cost matrix로 만들게 됩니다.  

<center><img src="/assets/images/allox/allox_06.jpg" width="40%" height="40%"></center>  

위 그림처럼 기존에 예시를 들었던 matrix P는 작업의 개수가 3개이고, k-th last job이 만들어질 수 있는 k값이 각각 곱해져서 matrix Q와 같은 cost matrix를 만들어내게 됩니다. 각 행은 matrix P와 마찬가지로 작업을 나타내고, 열은 configuration과 k-th last job의 k값의 pair에서 나타나는 processing time이 됩니다. 이제 남은 일은 이렇게 만들어진 cost matrix에서 각 행에서 다른 행과 겹치지 않는 열을 고르고, 고른 값들의 합을 최소로 만들도록 하면 그 configuration들이 각 작업에 대한 schduling configuration이 됩니다. AlloX는 이 cost matrix의 solution을 찾기 위해서 각 작업과 configruation을 두 그룹으로 나누어 bipartite graph(이분 그래프)로 만들고 두 그룹 사이의 edge를 matrix의 element로 결정합니다. 이렇게 만들어진 그래프를 Hungarian method를 통해 해결하면 O(n^3) time complexity 안에서 solution을 발견할 수 있게 됩니다. (Hungarian method에 대한 자세한 알고리즘은 이 논문에서 나오지 않습니다. 이 알고리즘에 대한 내용은 <a href="https://gazelle-and-cs.tistory.com/29">여기</a>에 더 자세히 설명이 되어 있습니다.)  
여기까지 AlloX는 각 작업들이 시작과 동시에 도착한다는 가정 하에 스케줄링 방법을 찾았습니다. 그런데 만약 scheduling 도중에 새로운 작업이 들어오게 되면 AlloX는 어떻게 처리하게 되는 것일까요? 이 경우, AlloX는 Delay Matrix를 도입하여 해결합니다. 기존의 processing matrix에 추가적인 행을 도입하고, 새로운 작업이 도착한 시간과 각 configuration에서 가장 빨리 사용될 수 있는 time의 차이값을 추가되는 행의 값으로 사용하게 됩니다.  
그렇다면 online arrival이 이렇게 되는건 좋은데, 만약 기존의 작업들에 비해 더 configuration이 좋은 작업들이 계속 들어와서 head of line 문제가 발생하게 된다면 어떻게 될까요? AlloX의 저자들은 이러한 fairness 문제가 발생할 경우, 처음에는 Dominant Resource Fairness (DRF)를 확장시켜서 해결하려 했습니다. 이를 이해하기 위해서는 AlloX와 같은 multi-resource 환경에서 fairness를 충족하기 위한 4가지 속성을 알아야 합니다. 이는 각각 Pareto Efficiency (PE), Sharing Incentive (SI), Envy Freeness (EF) 그리고 Strategy Proofness (SP)라는 이름으로 해당 논문에서 소개하고 있습니다.

1. Pareto Efficiency: 각 작업은 다른 작업의 performance를 해치지 않고서는 자신의 performance 향상이 불가능하다.
2. Sharing Incentive: 작업이 N개 만큼 있는 경우, 각 작업은 최소한 system resource의 1/N만큼 사용하는 성능이 보장되어야 한다.
3. Envy Freeness: 각 작업은 다른 작업의 throughput 향상을 위해 resource를 주는 것을 좋아하지 않는다.
4. Strategy Proofness: 거짓말을 통해 자신의 성능을 향상시키는 것은 불가능하다. (즉, 정직해야 한다.)

DRF의 경우, single configuration에서는 위 4가지를 만족하지만, multi-configuration 환경에서는 이것이 불가능 했습니다. 따라서 처음 계획은 가장 짧은 processing time을 갖는 작업의 resource configuration을 선택하고, DRF를 이용해 interchangeability를 무시하고 resource를 할당하는 방식을 사용하려 했습니다. (이 방법이 저자들이 앞서 말한 DRF를 확장시킨 방식이라고 합니다.) 하지만 이 방법을 사용하는 경우, 가장 짧은 processing time의 작업을 고르는 행동이 PE, SI를 위배하거나 SP를 위배하게 되는 방식입니다.  
예를 들어 N명의 사용자가 모두 같은 작업을 하려고 가정을 해봅시다. 이 때, 사용자들이 하려는 작업은 딥러닝 작업으로, GPU configuration에서 하는 것이 CPU보다 근소한 차이로 빠르다고 가정을 하게 되면, 위 방식대로 확장된 DRF를 사용하려고 하면 모든 사용자들은 GPU를 선택하려 할 것입니다. 이렇게 되면 GPU는 모든 사용자에게 공평하게 나누어지게 되지만 CPU는 그 어느 사용자도 선택되지 않았으므로 idle 상태가 될 것입니다. 만약 memory bottleneck 같은 특수한 상황이 일어나지 않는다면 이는 equal sharing을 한 것보다 성능이 더 좋지 않으므로 SI에 위배가 됩니다. 만약 어떤 사용자가 CPU가 비어있는 것을 눈치채고 CPU를 사용하기 위해 거짓 할당을 요청하게 되면 이는 SP에 위배되는 행동이 됩니다.  
그래서 AlloX의 저자들은 Fairness 문제를 해결하기 위해 DRF 방식을 processing time을 기준으로 하는 것이 아니라 progress라는 것을 도입하여 해결합니다. AlloX는 쿠버네티스를 이용해 resource management를 하고, 이를 통해 CPU, GPU, memory등 클러스터의 전체 resource 양을 알아냅니다. 그리고 주기적으로 각 작업에 대해 해당 작업이 각 configuration을 사용할 때 얼마 만큼의 demand가 필요한지, memory는 얼마나 사용해야 하는지, 해당 configuration을 사용할 경우의 processing time을 조사합니다. 그리고 이를 이용해 각 configuration마다 dominant share를 계산하게 됩니다. AlloX에서 제안한 각 configuration별 dominant share를 계산하는 방식은 아래 그림과 같습니다.  

<center><img src="/assets/images/allox/allox_07.jpg" width="50%" height="50%"></center>

각각의 dominant share가 정해지면 CPU의 processing time과 GPU의 processing time 사이의 비율을 dominant share에 곱하여 progress라 불리는 값을 만들게 되고, 이를 각 작업마다 모두 구하게 됩니다. 이렇게 완성된 progress 값들을 가지고 AlloX는 미리 정의된 0과 1 사이의 값을 가지는 α값에 전체 작업의 수를 곱하여 먼저 스케줄링 될 작업들을 선별해냅니다. α값을 통해 선별한 작업들 사이에서 progress를 비교하고, 더 작은 progress를 가진 작업부터 먼저 우선권을 주게 됩니다. AlloX는 이러한 방식으로 Fairness를 각 작업들 사이에 부과하여 HOL 문제를 해결하게 됩니다. 이렇게 스케줄링된 작업들은 Placer 단계에서 쿠버네티스의 Kubelet container에서 실행 됩니다.  
완성된 AlloX의 성능을 비교하기 위해서 저자들은 다음과 같은 6가지의 Baseline을 두고 실험을 했습니다.

1. Equal Share (ES with SJF)
2. DRFF (online DRF with FCFS)
3. DRFS (DRF with SJF)
4. DRFA (DRF with average speedup rate)
5. SRPT (Shortest remaining processing time with preemption)
6. AlloX+ (Heuristic version of AlloX)

위 baseline 중에서 주목해야 할 것은 SRPT인데요, 이는 시스템이 preemption을 허용하는 경우로 제일 이상적이지만 실제 시스템에서 (특히 쿠버네티스에서) 지원되지 않는 방식입니다. 따라서 스케줄러의 결과가 SRPT와 근접하게 된다면 이는 이상적인 성능을 가진 스케줄러에 가깝다고 볼 수 있습니다. 추가적으로 AlloX+의 경우는 waiting time이 특정 time-out heuristic을 넘어가게 되면 priority를 가지는 방식을 의미합니다. 이에 대한 결과는 글 아래에 논문 링크에서 확인할 수 있습니다.  
끝으로 논문에서는 AlloX의 몇가지 operational issue에 대한 내용을 담고 있습니다.

1. 스케줄링을 해야 하는 주기가 짧아지게 되면 Hungarian method를 여러번 실행하게 되면서 이로 인한 overhead가 발생하게 됩니다. AlloX의 저자들은 이를 parallel programming이나 divide and conquer 또는 Heuristic을 도입하여 개선할 수 있다고 주장합니다.
2. 딥러닝 작업에 대한 profiling의 경우, 만약 주어진 작업이 너무 긴 시간이 소요되는 작업이라면 3%의 작업만 돌려도 지나치게 오랜 시간이 소요될 것입니다. 이 경우에 time-out을 도입하여 profiling을 끝까지 하지 않고 해당 작업은 너무 오래 걸리는 작업으로 간주하게 됩니다.
3. 모든 작업을 GPU에서 하게 되더라도 적절한 실행을 위해서는 CPU가 필요하게 됩니다. 따라서 AlloX에서는 GPU 작업을 위해 일부 CPU 코어를 전담용으로 할당하여 이 문제를 해결했다고 주장 합니다. 클러스터 내 CPU 코어의 수가 많을수록 이 문제는 성능에 영향을 별로 미치지 않게 된다고 합니다.
4. Fairness에서 α값이 작은 경우, utilization이 낮아지는 문제가 발생할 수 있습니다. α값이 작아지게 되면 일부 작업들은 더 나은 resource를 얻기 위해 오랜 시간을 기다리게 됩니다. AlloX에서는 이를 해결하기 위해 일시적으로 α값을 증가시켜서 더 많은 작업을 일시적으로 할당하게 할 수 있습니다.
5. 끝으로 특정 resource에 문제가 생겨서 사용할 수 없게 되는 경우 입니다. 이 경우엔 쿠버네티스를 사용함으로써 주기적으로 resource의 상태를 체크하고 클러스터의 각 노드를 업데이트 합니다. 그렇기 때문에 이에 대한 추가적인 overhead는 발생하지 않는다고 합니다.  

결론적으로 AlloX는 다른 baseline 스케줄러들에 비해 average completion time을 95%까지 줄일 수 있고 이러한 성능은 SRPT 스케줄링에 비견될만 하다는 것으로 논문 내용은 끝나게 됩니다. 하지만 AlloX의 타겟은 single resource를 사용하는 작업들이고 만약 distributed 작업이 들어오게 된다면 다른 스케줄링 방식이 필요하게 됩니다. (논문에서도 이러한 단점은 후에 future work로 두었다고 되어 있습니다.)  

\[논문 및 그림 출처]: <a href="https://dl.acm.org/doi/abs/10.1145/3342195.3387547"> AlloX: compute allocation in hybrid clusters</a>, Tan N. Le, Xiao Sun, Mosharaf Chowdhury, and Zhenhua Liu, Eurosys'20, <a href="https://github.com/lenhattan86/allox">Github</a>
