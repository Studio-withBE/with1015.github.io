---
title: "Hipster: Hybrid Task Manager for Latency-Critical Cloud Workloads"
author: Hyunjoon Jeong
layout: post
category: review
---

본 리뷰는 HPCA'17에 게재된 논문 "Hipster: Hybrid Task Manager for Latency-Critical Cloud Workloads"를 읽고 내용을 간단히 정리하고자 쓰였습니다.
(틀린 내용이나 잘못 이해하고 있는 부분이 있다면 지적 부탁 드리겠습니다.)  

Hipster는 가장 먼저 2013년에 미국의 데이터 센터들이 미국 전역에서 생산된 전력의 2.2%를 사용하고 있다는 이야기에서 출발합니다. 데이터 센터들의 막대한 전력 소모는 향후 10년간 더 증가할 것으로 보여지며, 이러한 추세는 대다수의 workload들이 사용자의 요구에 맞추기 위해 엄격한 QoS를 요구하기 때문에 성능에 대한 요구 증가로 인해 전력 소비의 감소는 더이상 어려울 것이라고 소개합니다. 따라서 Hipster의 연구진들은 workload들이 요구하는 QoS 성능을 만족시키면서 동시에 전력 소모 또한 감축시킬 수 있는 시스템을 고안하고자 했고, 그러한 시스템이 Hipster라고 소개합니다.  
Hipster에 대해 설명하기에 앞서, 가장 먼저 ARM의 heterogeneous computing achitecture인 big.LITTLE에 대해 알아야 합니다. ARM architecture에는 상대적으로 전력 소모가 많지만 성능이 뛰어난 Big Core와, 그에 비해 성능은 떨어지지만 전력 소모가 덜한 Small Core (LITTLE core)가 있습니다. big.LITTLE 구조는 이러한 core를 동시에 탑재하고, 일반적으로 두 종류의 코어 중 한 쪽만이 활성화되어 작동하게 됩니다. 또한 코어들은 동일한 메모리 영역을 사용하기 때문에, 작업들은 상황에 따라 big core나 LITTLE core에 동적으로 할당 됩니다.  

<center><img src="/assets/images/hipster/hipster_01.jpg" width="60%" height="60%"></center>  

위 그림은 Google의 데이터 센터에 로드되는 Web-search의 QPS(Query Per Second)를 ARM Juno R1 플랫폼에서 2개의 Big core에 할당했을 때, 서버의 전력 사용 분포를 나타낸 것입니다. 위 그림에서 볼 수 있듯이, QPS는 최대 로드 용량의 5%까지 떨어지는데 비해, 전력 소모는 60%에서 머물고 있습니다. 이로인해 학계와 산업계에서는 시스템의 전력 소모는 활용률과 비례하고 더 완벽한 비례성을 목표로 하고 있다고 합니다. Hipster의 저자들은 에너지 효율을 위해 DVFS(Dynamic Voltage and Frequency Scaling)을 이기종 서버에 결합할 수 있다고 주장합니다.

<center><img src="/assets/images/hipster/hipster_02.jpg" width="80%" height="80%"></center>  

위 그림은 Octopus-Man이라는 baseline을 사용한 RPS(Request Per Second)/Watt와 QPS/Watt를 여러 core configuration과 DVFS 세팅을 기준으로 나타낸 그래프 입니다. HetCMP는 Hipster의 저자들이 연구한 DVFS와 이기종의 core type을 합친 heterogeneous architecture를 의미합니다. 각 architecture는 각각 Memcached와 Web-Search에 대한 벤치마크를 다른 여러 load level별로 실행하였습니다. 60% 이하의 낮은 load에서 두 벤치마크는 동일하게 4개의 small core를 사용했고, 마찬가지로 95% 이상의 높은 load에서는 2개의 big core를 이용하였습니다. 낮은 로드의 경우, 성능이 낮은 코어를 사용해도 QoS를 충분히 충족시킬 수 있기 때문에 전력 사용이 낮은 코어를 사용하게 됩니다. 하지만 load가 증가하게 되면서 HetCMP는 낮은 성능의 코어에서 특정 DVFS의 최적화된 core configuration으로 전환하게 됩니다. 그에 비해 baseline으로 사용된 Octopus-Man은 latency를 충족시키기 위해 load가 증가할 경우 성능이 높은 코어의 조합으로만 전환하게 됩니다. 이 경우, QoS는 충족할 수 있게 되지만 전력 소모가 높아지게 되면서 RPS/Watt와 QPS/Watt가 떨어지게 됩니다. 이를 통해 기존의 heuristic을 적용한 Octopus-Man은 데이터 센터가 하루동안 일반적으로 겪에 되는 중간 정도의 load(60~95%)에는 전력 소모를 고려하지 않았음을 Hipster 저자들은 지적합니다.  

<center><img src="/assets/images/hipster/hipster_03.jpg" width="80%" height="80%"></center>  

위 그림은 Memcached와 Web-search에서 다른 load capacity를 가질 때의 최적화 된 core mapping에 대한 state transition(왼쪽 그래프)와 각 state machine을 바꿔서 적용했을 때의 energy efficiency를 정규화한 지표(오른쪽 그래프)를 나타냅니다. 위 두 그림을 통해 결국 Hipster의 저자들은 다음과 같이 결론을 내렸습니다.  

1. 중간 단계의 load capacity에서 big core와 small core를 적절하게 섞어 사용하는 것이 최적이다.
2. 서로 다른 latency-critical application은 다양한 state-machine mapping에 대해 이점이 존재한다.

즉, 각 workload 마다 특수성이 존재하고 이로 인해 최적화 된 core mapping이 다르기 때문에, 이러한 heterogeneous architecture에서 energy efficiency와 DVFS 특성의 장점을 살리고, load change가 발생할 때마다 여러 core level에서 QoS를 만족시킬 수 있는 application이 필요하다고 주장합니다. 따라서 Hipster는 이러한 motivation을 가지고 구현 되었다고 소개합니다.  

Hipster가 어떤 원리로 작동하는지를 소개하기 전에, 저자들은 Hipster에 2가지 variant model이 있다고 설명합니다.  

1. HipsterIn: interactive workload를 타겟으로 하며, latency-critical workload에 자원을 할당하는 동시에 시스템의 전력 소모를 최소화 하는 것이 목적인 모델입니다.
2. HipsterCo: collocated workload를 타겟으로 하며 latency-critical workload와 batch workload를 동시에 사용하여 서버의 활용률을 높이는 것이 목적인 모델입니다.

Hipster는 위와 같은 목적을 달성하기 위해서 core mapping에 MDP(Markov Decision Process)를 이용한 강화 학습을 도입하였습니다. Hipster의 구조에 대한 이해를 돕기 위해 MDP에 대해 간략히 소개를 하자면, MDP는 시간에 따른 전체 reward를 최대한 많이 얻는 것을 목표로 하는 강화학습 방식 입니다. 지난번 Ray 논문에서도 나왔던 것처럼 MDP는 state와 action을 통해 reward를 계산하며, 미래에 대한 보상일 수록 discounting factor가 적용 됩니다. Hipster의 경우, state, reward, action이 나타내는 것은 다음과 같습니다.

1. State: 이전 time interval에 대한 latency-critical workload의 현재 load된 양을 의미합니다.
2. Action: 다음 time interval에 대해 사용될 configuration (즉, core mapping이나 DVFS 세팅)
3. Reward: 주어진 최적화 방식에 따른 목표 대비 QoS level에 의해 결정됩니다. 예를들어 HipsterIn의 경우 QoS 대비 시스템의 전력 소모가, HipsterCo의 경우엔 batch workload의 throughput이 보상이 됩니다.

Hipster는 QoS 목표와 input load, 그리고 최적화를 위한 metric이 주어지면 lookup table을 관리하는 것으로 가장 최적화된 configuration과 DVFS 세팅을 학습하게 됩니다. 또한 Hipster는 강화학습 뿐만 아니라 학습 단계 도중에 heuristic을 이용하는 hybrid RL을 이용합니다. Hipster가 이러한 hybrid RL을 이용하는 이유는 강화 학습의 고질적인 문제로 알려진 Exploitation-Exploration Dilemma 때문입니다. 이를 간략히 요약하자면, 강화 학습이 어떠한 시점에서 추가적인 탐색을 해야 더 많은 보상을 얻을 수 있는지, 아니면 현재 가진 policy가 최대 보상을 보장하는지 알 수 없기 때문에 발생하는 문제 입니다. Hipster의 저자들은 이러한 딜레마를 heuristic 도입을 통해 해결할 수 있고, 그로 인해 얻을 수 있는 효과를 다음과 같이 요약했습니다.

1. 학습 단계에서 heuristic을 사용하지 않으면 시스템은 무작위로 많은 결정을 내리게 될 것이고 이는 QoS violation을 더 자주 일으키게 된다.
2. 풀고자 하는 문제의 복잡도가 증가할 수록 Hipster의 lookup table이 매우 크게 증가할 것이다. 이를 방지하기 위해 heuristic을 도입하고, 이를 이용한 학습 단계는 QoS violation이 발생하는 경우의 수를 감소 시켜주고 학습 단계 도중에도 합리적인 해결책을 제시해준다.
3. Hipster의 heuristic은 실행 시작 후, 학습이 되지 않은 상태에서도 QoS를 개선 시켜주고, batch workload와 혼합된 모델로 변경된 경우에도 재사용이 가능하다.

결과적으로 Hipster의 시스템 구조는 다음 그림과 같이 구현이 되었습니다.  

<center><img src="/assets/images/hipster/hipster_04.jpg" width="50%" height="50%"></center>  

먼저 중앙에 QoS Monitor는 latency-critical workload나 batch workload로부터 application 단계의 QoS metric (예를 들면 RPS, QPS 같은 throughput이나 tail latency) 같은 성능에 대한 통계를 수집합니다. 또한 현재 latency-critical workload의 load양을 읽고 bucket 단위로 값을 묶는 역할을 합니다. 이러한 QoS Monitor는 HipsterCo의 경우, 프로파일링 툴을 이용해 구현했다고 밝혔고, 여기서 측정된 데이터는 thread-to-core mapping 결정에 사용된다고 합니다.  

그 다음은 Heuristic Mapper 입니다. Heuristic Mapper는 피드백 루프를 사용하는 state machine으로 현재 QoS 상태에 따라 다음 state로 전이를 결정합니다. 학습 초기에는 사전에 미리 정의된 순서에 따라 state를 결정합니다. 이러한 순서는 일반적으로 가장 높은 energy efficiency부터 가장 낮은 방향으로 정렬되어 있다고 합니다. 만약 QoS가 violation에 가까워지게 되는 경우, Heuristic Mapper는 그 다음으로 높은 상태의 power state로 전이하게 되고, 반대로 QoS가 violation으로부터 멀리 떨어진 경우, 그 다음 낮은 상태의 power state로 전이하게 됩니다. 이러한 Heuristic Mapper는 학습 단계를 통해 낮은 load와 높은 load가 발생할 때, 각각 다른 DVFS 세팅과 core configuration으로 latency-critical workload를 resource에 매치 시킵니다. 만약 낮은 load라면 현재 상태보다 더 낮은 DVFS에서 small core를 이용할 것이고, load가 높아질 경우에 높은 DVFS에서 big core와 small core를 결합하여 사용하게 될 것입니다.  

Hipster는 시스템에서 MDP를 이용하기 위에 내부에 Lookup table 또한 도입하였습니다. Lookup table은 state와 해당 state에서 가능한 action을 통해 연산한 total discounted reward R값을 계산하여 저장하는 역할을 합니다. 만약 계산된 R값이 최적화된 core mapping의 보상에 거의 근접한다면 해당 R값이 가장 큰 total discounted reward임을 의미하게 되고, 이러한 보상 값을 적용한 action이 현재 load에서 최적의 core mapping과 DVFS 세팅을 가질 수 있다는 것을 의미합니다. 이러한 Lookup table은 새로운 state와 기존의 state, 그리고 보상 탐색의 metric 별 특정한 공식에 의해 계산이 되어 업데이트 됩니다.

<center><img src="/assets/images/hipster/hipster_05.jpg" width="40%" height="40%"></center>  

위 그림은 Hipster에서 보상이 계산되는 과정을 나타내는 pseudo-code 입니다. 이러한 계산 과정은 QoS monitor에서 일정 시간 간격마다 실행되는 모니터링이 끝난 후에 실행 됩니다. 모니터링을 통해 Hipster는 현재 실행중인 interactive workload의 tail latency를 측정합니다. 이 때 tail latency의 범위는 전체 latency 구간에서 95% ~ 99%에 해당하는 영역을 의미하고, 위 pseudo-code에서 QoScurr에 해당됩니다. 마찬가지로 QoS Monitor는 현재 로드가 소모하는 전력량을 측정합니다. 이는 위 알고리즘에서 Power와 TDP (Thermal Design Power)라는 값으로 측정이 됩니다.  
우선 4번째 줄에서 Hipster는 QoSreward라는 값을 현재 QoS를 의미하는 QoS_curr과 목표 QoS인 QoS_target의 비율로 나타냅니다. 따라서 만약 QoSreward가 1보다 작을 수록 현재 QoS는 QoS_target을 만족함과 동시에 더 나은 QoS를 보이고 있다는 뜻이 됩니다. Hipster에서는 이러한 상태를 QoS earliness라고 표현하고 있습니다. 이 경우, Hipster는 6번째 줄부터 9번째 줄까지의 알고리즘처럼 positive reward를 적용하게 되고, 이는 전력 소모를 줄이거나 batch workload throughput 개선을 위한 지표로 쓰이게 됩니다.  
9번째 줄의 경우, 7번째 줄의 보상 적용과는 다르게 0과 1 사이의 랜덤값을 빼주는 부분이 있는데요, 이 부분을 Hipster에서는 Stochastic penalty라고 부릅니다. 만약 현재 QoS가 목표를 달성하여 QoS earliness 상태가 되었을 때, core mapping은 현재보다 더 저전력을 소모하는 상태로 전이를 시도할 것입니다. 그러나 이렇게 core configuration 상태를 전이했을 때, QoS가 목표를 달성하지 못하게 되는 일이 일어날 수 있습니다. 6번째 줄의 QoS_D는 QoS danger zone을 형성하여 core configuration이 전이하였을 때 발생할 수 있는 QoS violation을 미리 감지하는 역할을 합니다. 이를 이용해 Hipster는 QoS earliness를 2가지로 분류하고, QoS danger zone에 위치한 earliness의 경우엔 Stochastic penalty를 적용하여 configuration 탐색의 확률을 낮추는 역할을 하게 됩니다.
반대로 QoS_reward가 1보다 크게 되는 경우, 현재 QoS가 QoS 목표보다 큰 값을 가지고 있음, 즉 QoS_curr이 QoS_target을 만족하지 못한다는 것을 의미하게 됩니다. 이러한 경우를 QoS tardiness라고 부르게 되며, 이는 10번째 줄과 11번째 줄에 의해 negative reward를 적용하게 됩니다.  
12번째 줄부터 15번째 줄까지는 HipsterIn과 HipsterCo의 보상 계산을 의미합니다. HipsterCo의 경우, batch workload가 존재하기 때문에 13번째 줄의 보상 계산을 선택하게 됩니다. 이 경우, 보상은 big core의 IPS (Instruction Per Second)와 small core의 IPS에 의해 증가하게 됩니다. 따라서 보상값은 batch workload의 throughput에 거의 비례하게 됩니다. 반면에 HipsterIn의 경우, TDP와 시스템의 전력 소모 간의 비율에 의한 보상이 추가됩니다. 즉, energy efficiency를 측정하여 이를 보상에 반영하며, Power_reward의 값이 높아질수록 낮은 에너지를 소비한다는 것을 의미하게 됩니다.  
마지막으로 16번째 줄은 모든 계산이 끝나면 reward를 lookup table에 업데이트 하는 역할을 합니다. 여기에는 discounting factor를 통해 단기적인 보상에 대한 선호도를 조정하고 (discounting factor인 감마 값이 0에 가까울수록 단기 보상을 더 선호) learning rate를 통해 lookup table의 업데이트 속도를 조절합니다. learning rate의 경우, 알파 값이 1에 가까울수록 빠르게 학습하고 최근에 경험한 학습을 선호하게 되지만 noise에 취약해진다는 단점이 존재합니다. Hipster 논문의 경우, 실험에서 discounting factor를 0.9, learning rate를 0.6으로 설정하여 사용했다고 합니다.  
Hipster는 exploitation 단계에서 lookup table을 사용하여 load에 기반해 core mapping과 DVFS 세팅을 선택합니다. 또한 미리 사전에 설정된 시간 단위마다 Hipster는 학습 단계와 exploitation 단계를 동적으로 바꾸게 됩니다. Exploitation 단계에 대한 알고리즘은 다음 그림을 따릅니다.  

<center><img src="/assets/images/hipster/hipster_06.jpg" width="40%" height="40%"></center>  

우선 Hipster는 exploition phase에서 현재 state에서 가장 큰 total discounted reward를 가지는 configuration을 7번째 줄에서 가져오게 됩니다. 이후 8번째 줄부터 13번째 줄까지 Hipster variant model에 따라 달라지는 configuration mapping을 수행하게 됩니다. 일반적으로 latency-critical workload와 batch workload를 모두 실행하는 경우 shared resource 경쟁으로 인해 높은 load에서 QoS 저하가 발생합니다. 이를 방지하기 위해 HipsterCo는 8-9번째 줄에서 QoS 목표를 만족하는 동시에 throughput-oriented workload의 throughput을 극대화하기 위해 batch workload에 먼저 남는 core를 할당하게 됩니다. 이후 10-11번째 줄에서 볼 수 있듯이, latnecy-critical 작업이 특정 core에 할당되어 있는 경우, 다른 core는 가장 높은 DVFS로 설정되어 batch workload에 사용됩니다. HipsterIn의 경우, 7번째 줄에서 QoS target을 충족하면서 동시에 전력 소비를 최소화 하기 위해 table에서 reward가 가장 높은 configuration을 찾은 뒤, 이후 12-13번째 줄에서 나머지 core에 대해 DVFS 설정이 가장 낮은 값으로 바뀌게 됩니다.  
이후 15번째 줄에서 latency-critical workload는 bucket을 통해 분류되고, 16번째 줄에서 이전의 reward caculation algorithm을 통해 Lookup table을 업데이트 하게 됩니다. 만약 그럼에도 QoS가 보장되지 않는 경우, 18번째 줄을 통해 learning phase로 되돌아가게 됩니다.  

Hipster는 시스템의 각각의 모듈을 user space에서 구현하였습니다. 또한 Linux에서의 하드웨어 지원은 최소한으로 사용한다고 합니다. QoS Monitor의 경우, 전력 측정이나 런타임 통계를 측정하기 위해 분리된 프로세스를 따로 사용 했다고 합니다. 특히 HipsterCo의 경우, perf 같은 모니터링 툴을 사용하거나 Docker와 Kubernetes에서 지원하는 프로파일링 툴을 사용 했다고 합니다. Mapper module의 경우, Linux의 sched_setaffinity를 core mapping에 사용하였으며, DVFS 조절에는 acpi-cpufreq를 사용했다고 합니다. Lookup table의 경우, Hipster의 초기 단계에서는 open addressing 방식을 이용해 python dictionary를 사용했다고 합니다. 결과적으로 Hipster의 구현에서는 간단한 control flow algorithm과 함께 main memory를 사용하기 때문에 런타임 오버헤드를 무시할 수 있었다고 전합니다.  

결론적으로 Hipster는 MDP와 heuristic을 결합한 hybrid RL의 형태로 hetergeneous core configuration과 DVFS 세팅에 맞추어 latency-critical workload와 batch workload를 낮은 전력으로도 관리할 수 있도록 하였습니다. Lookup table의 보상 계산을 통해 Hipster는 QoS, 전력 소비, performance history를 학습하여 최적의 configuration mapping을 찾을 수 있었고, HipsterIn에서 이는 critical workload만 시스템에서 사용했을 때 최대 13%까지 에너지 소비를 줄일 수 있었고, HipsterCo의 경우, 2.3배 가량 throughput 향상을 보일 수 있었습니다. 자세한 실험 내용은 아래 링크의 논문에서 찾아보실 수 있습니다.

\[논문 출처 및 그림]: <a href="https://ieeexplore.ieee.org/document/7920843">Hipster: Hybrid Task Manager for Latency-Critical Cloud Workloads</a>, Rajiv Nishtala, Paul Carpenter, Vinicius Petrucci, Xavier Martorell, HPCA'17
