---
title: "Ray: A Distributed Framework for Emerging AI Applications"
author: Hyunjoon Jeong
layout: post
category: review
---
본 리뷰는 OSDI'18에 게재된 논문 "Ray: A Distributed Framework for Emerging AI Applications"를 읽고 내용을 간단히 정리하고자 쓰였습니다.  
(틀린 내용이나 잘못 이해하고 있는 부분이 있다면 지적 부탁 드리겠습니다.)  

Ray를 소개하기 전, 먼저 강화 학습(Reinforcemnet Learning)이 어떤 방식으로 이루어지는지 알아야 합니다.  
강화 학습은 비지도학습으로 label이 존재하지 않고 주변의 환경과 상호작용을 통해 문제를 해결합니다.

<center><img src="/assets/images/ray_01.jpg" width="50%" height="50%"></center>  

위 그림은 강화 학습의 Agent와 환경이 어떻게 상호작용이 되는가를 나타낸 그림입니다. Agent는 환경을 보고 action을 하는 주체로, 환경의 state를 보고 reward를 계산합니다. 이러한 reward를 이용해 지속적으로 policy를 수정해 action을 결정하게 되고 수정된 policy를 문제 상황에 적용하여 해결하게 됩니다. Ray는 이러한 강화학습을 하기 위해서 시스템에 아래와 같은 3가지 요소가 필요하다고 합니다.  

1. 시뮬레이션이나 다른 물리적 환경에 의해 발생한 데이터를 통해 policy를 수정하는 "training"
2. 현재 policy에 state를 적용했을 때, agent의 action을 도출하는 "serving"
3. policy를 평가하고 여러 action 선택지를 보면서 장기적으로 어떤 결과가 나타날지 탐색하는 "simulation"

Ray는 위 3가지 요소를 언급하면서 기존의 framework들은 위 3가지 요소를 시스템에 한번에 갖추고 있지 않다고 주장합니다. 예를들어 MapReduce, Spark, Dryad 같은 경우, 이들은 모두 Bulk-synchronous parallel system으로 training에 대한 작업은 가능하지만 simulation과 serving은 불가능 하다고 합니다. Tensorflow와 MXNet도 마찬가지로 이들은 Distributed deep learning framework이기 때문에 training은 가능하지만 simulation과 serving 역시 불가능 하다고 언급되어 있습니다.  
추가적으로 Ray에서는 Trajectory라는 state와 reward의 튜플이 등장합니다. Trajectory는 기존의 policy에 현재 환경의 state를 도입한 action과 이를 simulation한 결과를 통해 만들어지며 (Ray에서는 이 과정을 Roll-out이라고 표현했습니다.), 이러한 trajectory의 집합을 통해 policy를 업데이트 합니다. 이러한 policy 업데이트를 하기 위해서는 위에서 언급한 training, serving, simulation의 과정이 모두 필요하게 됩니다. 따라서 Ray에서는 시스템이 다음과 같은 능력들이 다시 요구 된다고 합니다.

1. 초당 백만 단위의 heterogeneous task들을 milisecond 단위의 latency 내에서 처리할 수 있어야 합니다.
2. Stateless, Stateful computation을 지원하기 위한 flexible한 처리 모델이 필요합니다.
3. 계산 결과는 미리 알 수 없고, 그 결과가 나중의 계산에도 영향을 미치기 때문에 dynamic computation이 가능해야 합니다.

따라서 Ray의 목표는 위 3가지 workload와 3가지 시스템 requirements를 하나의 application에 통합하는 것을 통해 강화학습을 적용하자는 것이라 할 수 있습니다.  
이를 Ray는 dynamic task graph computation model 위에 stateful computation을 담당하는 Actor와 stateless computation을 담당하는 Task-parallel programming을 디자인하는 것으로 구현하고자 했습니다.  

<center><img src="/assets/images/ray_02.jpg" width="50%" height="50%"></center>  

Ray의 Programming model은 worker를 stateless worker와 stateful worker로 나누는 것을 통해 디자인 되었습니다. 두 worker 모두 remote function을 통해 원격으로 다른 노드에서 함수를 실행하는 것이 가능합니다. Task는 remote function을 통해 원격으로 stateless worker에서 실행이 됩니다. 반대로 stateful worker는 stateful computation을 실행하며, 이를 실행시키는 것을 Actor라고 부릅니다. Task와 Actor는 위 그림과 같이 서로 trade-off를 가지고 있습니다. Task와 Actor는 remote function을 이용해 원격으로 실행될 수 있다는 점과, non-blocking이라는 점을 이용해 parallelism을 Ray가 지원 할 수 있고록 합니다. 추가적으로 Actor는 자신이 처리한 결과를 다른 Actor에게 넘겨주는 serialization 역할을 합니다. serialization을 통해 전달받은 Acotr는 이전 Actor의 method를 사용할 수 있습니다.

<center><img src="/assets/images/ray_03.jpg" width="80%" height="80%"></center>  

위 그림은 Ray의 API method를 보여줍니다. 위에서 언급하였듯, Task와 Actor는 remote function을 통해 실행이 되면서 future를 반환합니다. get과 wait은 양쪽이 가지는 non-blocking을 제어하고 동기화 시키기 위해 설계되었습니다.  

<center><img src="/assets/images/ray_05.jpg" width="60%" height="60%"></center> 

위 그림은 Ray의 policy training에 대한 computation model입니다. 그림에서 A와 T는 각각 Actor와 Task를 의미합니다. 처음 training policy가 생성되고 나면 simulator actor 또한 생성됩니다. 위 예시 그림의 경우, training policy는 simulator actor가 2개 생성 되었습니다. 생성된 simulator는 rollout을 통해 policy를 업데이트 합니다. 이 때, 각 simulator actor는 rollout을 생성할 때마다 stateful edge를 통해 각 버전들이 serialization으로 연결됩니다. 이러한 stateful의 특성을 Ray에서는 data lineage를 유지한다고 표현하며, 이를 이용해 Ray는 policy update 도중 failure 발생 시 recovery에 이를 사용할 수 있습니다.  

<center><img src="/assets/images/ray_06.jpg" width="60%" height="60%"></center> 

Ray는 위와 같은 예시 코드를 통해 computation model을 실제 코드에 적용하였습니다. train_policy 함수의 마지막 return에서는 Ray의 get API를 통해 모든 simulator의 업데이트 활동을 기다리고 생성되고 업데이트 된 policy의 동기화를 기다리게 됩니다.  

<center><img src="/assets/images/ray_07.jpg" width="60%" height="60%"></center>

Ray의 architecture는 위 그림처럼 크게 Application layer와 System layer로 나뉘게 됩니다. Application layer는 각 노드마다 Actor, Driver, Worker를 필요에 따라 가지게 됩니다. 각 구성원들의 역할은 다음과 같습니다.

1. Driver: user program을 실행하는 process 입니다.
2. Worker: stateless process로, Driver나 다른 worker에 의해 실행된 task를 처리합니다. remote function이 선언되면 모든 worker로 함수가 자동적으로 publish 되고, worker는 task끼리 loacal state를 유지하지 않습니다.
3. Actor: 앞서 언급한 stateful process로, serialization에 의해 실행된 이전 method 실행 결과에 dependency를 가지고 있습니다. 마찬가지로 Driver나 다른 worker에 의해 인스턴화 됩니다.

System layer는 Global Control Store (이하 GCS), 분산 스케줄러 (distributed scheduler), 그리고 Distributed Object Store가 존재합니다.  

수정중...

\[논문 및 그림 출처] <a href="https://dl.acm.org/doi/10.5555/3291168.3291210">Ray: A Distributed Framework for Emerging AI Applications</a>, Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I. Jordan, and Ion Stoica, UC Berkeley. OSDI'18. <a href="https://github.com/ray-project/ray">Github</a>
