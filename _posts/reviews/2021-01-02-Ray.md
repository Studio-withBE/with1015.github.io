---
title: "Ray: A Distributed Framework for Emerging AI Applications"
author: Hyunjoon Jeong
layout: post
category: review
---
본 리뷰는 OSDI'18에 게재된 논문 "Ray: A Distributed Framework for Emerging AI Applications"를 읽고 내용을 간단히 정리하고자 쓰였습니다.  
(틀린 내용이나 잘못 이해하고 있는 부분이 있다면 지적 부탁 드리겠습니다.)  

Ray를 소개하기 전, 먼저 강화 학습(Reinforcemnet Learning)이 어떤 방식으로 이루어지는지 알아야 합니다.  
강화 학습은 비지도학습으로 label이 존재하지 않고 주변의 환경과 상호작용을 통해 문제를 해결합니다.

<center><img src="/assets/images/ray_01.jpg" width="50%" height="50%"></center>  

위 그림은 강화 학습의 Agent와 환경이 어떻게 상호작용이 되는가를 나타낸 그림입니다. Agent는 환경을 보고 action을 하는 주체로, 환경의 state를 보고 reward를 계산합니다. 이러한 reward를 이용해 지속적으로 policy를 수정해 action을 결정하게 되고 수정된 policy를 문제 상황에 적용하여 해결하게 됩니다. Ray는 이러한 강화학습을 하기 위해서 시스템에 아래와 같은 3가지 요소가 필요하다고 합니다.  

1. 시뮬레이션이나 다른 물리적 환경에 의해 발생한 데이터를 통해 policy를 수정하는 "training"
2. 현재 policy에 state를 적용했을 때, agent의 action을 도출하는 "serving"
3. policy를 평가하고 여러 action 선택지를 보면서 장기적으로 어떤 결과가 나타날지 탐색하는 "simulation"

Ray는 위 3가지 요소를 언급하면서 기존의 framework들은 위 3가지 요소를 시스템에 한번에 갖추고 있지 않다고 주장합니다. 예를들어 MapReduce, Spark, Dryad 같은 경우, 이들은 모두 Bulk-synchronous parallel system으로 training에 대한 작업은 가능하지만 simulation과 serving은 불가능 하다고 합니다. Tensorflow와 MXNet도 마찬가지로 이들은 Distributed deep learning framework이기 때문에 training은 가능하지만 simulation과 serving 역시 불가능 하다고 언급되어 있습니다.  
추가적으로 Ray에서는 Trajectory라는 state와 reward의 튜플이 등장합니다. Trajectory는 기존의 policy에 현재 환경의 state를 도입한 action과 이를 simulation한 결과를 통해 만들어지며 (Ray에서는 이 과정을 Roll-out이라고 표현했습니다.), 이러한 trajectory의 집합을 통해 policy를 업데이트 합니다. 이러한 policy 업데이트를 하기 위해서는 위에서 언급한 training, serving, simulation의 과정이 모두 필요하게 됩니다. 따라서 Ray에서는 시스템이 다음과 같은 능력들이 다시 요구 된다고 합니다.

1. 초당 백만 단위의 heterogeneous task들을 milisecond 단위의 latency 내에서 처리할 수 있어야 합니다.
2. Stateless, Stateful computation을 지원하기 위한 flexible한 처리 모델이 필요합니다.
3. 계산 결과는 미리 알 수 없고, 그 결과가 나중의 계산에도 영향을 미치기 때문에 dynamic computation이 가능해야 합니다.

따라서 Ray의 목표는 위 3가지 workload와 3가지 시스템 requirements를 하나의 application에 통합하는 것을 통해 강화학습을 적용하자는 것이라 할 수 있습니다.  
이를 Ray는 dynamic task graph computation model 위에 stateful computation을 담당하는 Actor와 stateless computation을 담당하는 Task-parallel programming을 디자인하는 것으로 구현하고자 했습니다.  

<center><img src="/assets/images/ray_02.jpg" width="50%" height="50%"></center>  

Ray의 Programming model은 worker를 stateless worker와 stateful worker로 나누는 것을 통해 디자인 되었습니다. 두 worker 모두 remote function을 통해 원격으로 다른 노드에서 함수를 실행하는 것이 가능합니다. Task는 remote function을 통해 원격으로 stateless worker에서 실행이 됩니다. 반대로 stateful worker는 stateful computation을 실행하며, 이를 실행시키는 것을 Actor라고 부릅니다. Task와 Actor는 위 그림과 같이 서로 trade-off를 가지고 있습니다. Task와 Actor는 remote function을 이용해 원격으로 실행될 수 있다는 점과, non-blocking이라는 점을 이용해 parallelism을 Ray가 지원 할 수 있고록 합니다. 추가적으로 Actor는 자신이 처리한 결과를 다른 Actor에게 넘겨주는 serialization 역할을 합니다. serialization을 통해 전달받은 Acotr는 이전 Actor의 method를 사용할 수 있습니다.

<center><img src="/assets/images/ray_03.jpg" width="80%" height="80%"></center>  

위 그림은 Ray의 API method를 보여줍니다. 위에서 언급하였듯, Task와 Actor는 remote function을 통해 실행이 되면서 future를 반환합니다. get과 wait은 양쪽이 가지는 non-blocking을 제어하고 동기화 시키기 위해 설계되었습니다.  

<center><img src="/assets/images/ray_05.jpg" width="40%" height="40%"></center> 

위 그림은 Ray의 policy training에 대한 computation model입니다. 그림에서 A와 T는 각각 Actor와 Task를 의미합니다. 처음 training policy가 생성되고 나면 simulator actor 또한 생성됩니다. 위 예시 그림의 경우, training policy는 simulator actor가 2개 생성 되었습니다. 생성된 simulator는 rollout을 통해 policy를 업데이트 합니다. 이 때, 각 simulator actor는 rollout을 생성할 때마다 stateful edge를 통해 각 버전들이 serialization으로 연결됩니다. 이러한 stateful의 특성을 Ray에서는 data lineage를 유지한다고 표현하며, 이를 이용해 Ray는 policy update 도중 failure 발생 시 recovery에 이를 사용할 수 있습니다.  

<center><img src="/assets/images/ray_06.jpg" width="60%" height="60%"></center> 

Ray는 위와 같은 예시 코드를 통해 computation model을 실제 코드에 적용하였습니다. train_policy 함수의 마지막 return에서는 Ray의 get API를 통해 모든 simulator의 업데이트 활동을 기다리고 생성되고 업데이트 된 policy의 동기화를 기다리게 됩니다.  

<center><img src="/assets/images/ray_07.jpg" width="40%" height="40%"></center>

Ray의 architecture는 위 그림처럼 크게 Application layer와 System layer로 나뉘게 됩니다. Application layer는 각 노드마다 Actor, Driver, Worker를 필요에 따라 가지게 됩니다. 이들은 python 코드를 통해 구현되어 있으며 각 구성원들의 역할은 다음과 같습니다.

1. Driver: user program을 실행하는 process 입니다.
2. Worker: stateless process로, Driver나 다른 worker에 의해 실행된 task를 처리합니다. remote function이 선언되면 모든 worker로 함수가 자동적으로 publish 되고, worker는 task끼리 loacal state를 유지하지 않습니다.
3. Actor: 앞서 언급한 stateful process로, serialization에 의해 실행된 이전 method 실행 결과에 dependency를 가지고 있습니다. 마찬가지로 Driver나 다른 worker에 의해 인스턴화 됩니다.

System layer는 Global Control Store (이하 GCS), 분산 스케줄러 (distributed scheduler), 그리고 Distributed Object Store가 존재합니다. System layer는 C++ 코드로 구현되어 있습니다. GCS의 경우, Redis key-value store를 이용하였고, 분산 스케줄러의 경우, event-driven, single-threaded process로 이루어져 있습니다. 이들은 큰 규모의 object 전송을 위해서 다수의 TCP 연결로 구성되어 있습니다.  
Ray가 사용되는 환경은 일반적으로 초당 수백만개의 task가 동적으로 만들어지는 환경입니다. 따라서 GCS는 Ray의 fault tolerance와 latency를 낮게 유지하기 위해서 존재합니다. GCS 내부에는 각 테이블마다 object, task, function을 key-value 형태로 저장하게 됩니다. 따라서 다른 componet와 dependency가 없는 lineage storage를 구성할 수 있으며, 이 테이블을 통해 lineage를 읽어들이는 것으로 failure를 빠르게 해결할 수 있다고 주장합니다. 또한 GCS에서 object들은 메타데이터 형태로 저장이 되기 때문에 낮은 latency를 유지할 수 있습니다. 다른 lineage-based solution의 경우, master나 드라이버 같은 singel node에서 coarsed-grained parallelism에 초점이 맞추어져 있습니다. 그에 비해 Ray는 모든 component를 GCS에서 stateless로 저장하기 때문에 fine-grained의 특성을 가지면서 simulation이나 dynamic workload에 초점을 맞출 수 있게 됩니다.  

<center><img src="/assets/images/ray_08.jpg" width="50%" height="50%"></center>

그 다음, Ray의 System layer의 분산 스케줄러는 위 그림과 같습니다. 기존의 스케줄러의 경우, Spark나 Dryad의 centralizaed scheduler는 locality를 제공할 수 있지만 latency가 Ray의 환경에서 너무 크다는 단점이 존재합니다. 그에 비해 Sparrow나 Canary에서 쓰이는 distributed scheduler는 높은 scalability와 적절한 latency를 제공하지만 data locality를 고려하지 않았거나 스케줄러의 task들이 서로 독립적이라는 전제 하에 설계되었다고 합니다. 결론적으로 Ray는 locality 확보와 적절한 latency 내에서 locality 확보를 위해 위 그림과 같은 Bottom-up 방식의 분산 스케줄러를 제안하였습니다. Ray는 분산 스케줄러를 local 스케줄러와 global 스케줄러로 나누고 각 노드는 local 스케줄러로 작업을 보냅니다. 만약 local 스케줄러가 작업들로 가득 차게 될 경우, 이를 global 스케줄러로 전송하게 됩니다. global 스케줄러는 각 노드의 load와 받은 작업의 constraint를 고려하여 여유가 있는 다른 노드의 local 스케줄러로 작업을 보내게 됩니다. 만약 global 스케줄러에 병목현상이 발생하게 되는 경우, GCS는 같은 정보를 공유하는 global 스케줄러의 복제본을 만들어내고 이를 통해 scalability를 달성하게 됩니다.  

System layer의 마지막 구성원으로는 In-memory distributed object store가 있습니다. 이는 task의 latency를 최소화 시키고 data locality를 위해 사용됩니다. 만약 Task의 input이 local에 없는 경우 실행 전에 local object store에서 복제됩니다. 복제된 object는 자주 사용되는 data object로 인한 병목 현상을 예방하고 local memory에서 읽고 쓰는 작업만 하기 때문에 task의 실행 시간을 최소화 할 수 있다고 주장합니다. 또한 각 노드의 object store는 모든 stateless 계산의 input과 output을 shared memory를 통해 저장합니다. shared memory가 가득차게 되는 경우, LRU 방식으로 기존의 object를 disk로 방출하게 됩니다.  

결론적으로 Ray는 task-parallel과 actor programming model를 통합하였습니다. 이러한 scalable architecture 구성을 위해서 GCS와 Bottom-up 방식의 분산 스케줄러가 사용되었고, 초당 1.8백만개의 작업까지 처리할 수 있는 linear scalability를 달성하였다는 것으로 이 논문은 끝을 맺게 됩니다. Ray의 실험에 대한 결과와 end-to-end example에 대한 내용은 아래 논문 출처에서 확인하실 수 있습니다.  

\[논문 및 그림 출처] <a href="https://dl.acm.org/doi/10.5555/3291168.3291210">Ray: A Distributed Framework for Emerging AI Applications</a>, Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I. Jordan, and Ion Stoica, UC Berkeley. OSDI'18. <a href="https://github.com/ray-project/ray">Github</a>
