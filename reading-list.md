---
title: Reading List
subtitle: A list of paper I read
layout: "page"
icon: fa-book
order: 3
---

Interesting Field: cloud computing, machine learning platform, large scale distributed system

Deep learning
1. AlloX: Compute Allocation in Hybrid Clusters
2. Horovod: fast and easy distributed deep learning in TensorFlow
3. Accurate, large minibatch sgd: Training imagenet in 1 hour
4. Gpipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
5. PipeDream: Fast and Efficient Pipeline Parallel DNN Training
6. HetPipe: Enabling Large DNN Training on (Whimpy) Heterogeneous GPU Clusters through Integration of Pipelined Model Parallelism and Data Parallelism
7. Heterogeneity-aware Distributed Parameter Servers
8. More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server
9. Asynchronous Decentralized Parallel Stochastic Gradient Descent
10. A Unified Architecture for Accelerating Distributed DNN Training in Heterogeneous GPU/CPU Clusters
11. Hop: Heterogeneity-aware Decentralized Training
12. Heterogeneity-Aware Cluster Scheduling Policies for Deep Learning Workloads

RDMA & Infiniband
1. Accelerating TensorFlow with Adaptive RDMA-based gRPC
2. Fast Distributed Deep Learning over RDMA

Resource management field

1. Quasar: Resource-Efficient and QoS-Aware Cluster Management
2. Holistic VM Placement for Distributed Parallel Applications in Heterogeneous Clusters
3. Achieving Fairness-aware Two-level Scheduling for Heterogeneous Distributed Systems
4. Dominant Resource Fairness: Fair Allocation of Multiple Resource Types

Reinforce learning field

1. Hipster: Hybrid Task Manager for Latency-Critical Cloud Workloads
2. Retiarii: A Deep Learning Exploratory-Training Framework
3. Ray: A Distributed Framework for Emerging AI Applications

Disaggregation system field

1. Disaggregation and the Application
2. LegoOS: A Disseminated, Distributed OS for Hardware Resource Disaggregation
