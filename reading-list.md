---
title: Reading List
subtitle: A list of paper I read
layout: "page"
icon: fa-book
order: 3
---

Interesting Field: cloud computing, machine learning platform, large scale distributed system

1. AlloX: Compute Allocation in Hybrid Clusters
2. Horovod: fast and easy distributed deep learning in TensorFlow
3. Accurate, large minibatch sgd: Training imagenet in 1 hour
4. Gpipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism
5. PipeDream: Fast and Efficient Pipeline Parallel DNN Training
6. HetPipe: Enabling Large DNN Training on (Whimpy) Heterogeneous GPU Clusters through Integration of Pipelined Model Parallelism and Data Parallelism
7. Heterogeneity-aware Distributed Parameter Servers
8. More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server
9. Ray: A Distributed Framework for Emerging AI Applications
10. Retiarii: A Deep Learning Exploratory-Training Framework
11. Quasar: Resource-Efficient and QoS-Aware Cluster Management
12. Holistic VM Placement for Distributed Parallel Applications in Heterogeneous Clusters
13. Achieving Fairness-aware Two-level Scheduling for Heterogeneous Distributed Systems
14. Dominant Resource Fairness: Fair Allocation of Multiple Resource Types
15. Hipster: Hybrid Task Manager for Latency-Critical Cloud Workloads
